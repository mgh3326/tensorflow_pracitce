{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' RNN 신경망 구성하기 '''\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 전체 알파벳 정보를 저장합니다.\n",
    "char_arr = ['a', 'b', 'c', 'd', 'e', 'f',\n",
    "            'g', 'h', 'i', 'j', 'k', 'l',\n",
    "            'm', 'n', 'o', 'p', 'q', 'r',\n",
    "            's', 't', 'u', 'v', 'w', 'x',\n",
    "            'y', 'z']\n",
    "\n",
    "# a부터 z까지 인덱스를 붙여 저장합니다.\n",
    "num_dic = {n: i for i, n in enumerate(char_arr)}\n",
    "dic_len = len(num_dic)\n",
    "\n",
    "# 학습에 사용할 단어를 배열로 저장합니다.\n",
    "seq_data = ['such', 'case', 'dare', 'duel', 'late', 'carp',\n",
    "            'hall', 'java', 'fell', 'none', 'dark', 'doom',\n",
    "            'dark', 'dark', 'sqrt', 'them', 'them', 'here']\n",
    "\n",
    "def make_batch(seq_data):\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "    \n",
    "    for seq in seq_data:\n",
    "        # 단어의 처음 세 글자의 알파벳 인덱스를 구한 배열을 생성합니다.\n",
    "        input = [num_dic[n] for n in seq[:-1]]\n",
    "        # 마지막 글자의 알파벳 인덱스를 구합니다.\n",
    "        target = num_dic[seq[-1]]\n",
    "        # 입력값을 원-핫 인코딩 방식으로 변환합니다.\n",
    "        input_batch.append(np.eye(dic_len)[input])\n",
    "        # 출력값은 원-핫 인코딩이 아닌 인덱스 그대로 이용합니다.\n",
    "        target_batch.append(target)\n",
    "        \n",
    "    return input_batch, target_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' RNN 신경망 학습 모델 구성하기 '''\n",
    "\n",
    "# RNN 학습 오류 방지를 위해 그래프 리셋 함수를 호출합니다.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 모든 알파벳을 배경으로 3글자를 단계적으로 학습합니다.\n",
    "X = tf.placeholder(tf.float32, [None, 3, dic_len])\n",
    "# 출력 값으로는 인덱스 그 자체를 이용합니다.\n",
    "Y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "# 은닉층의 크기는 256입니다.\n",
    "W = tf.Variable(tf.random_normal([256, dic_len]))\n",
    "b = tf.Variable(tf.random_normal([dic_len]))\n",
    "\n",
    "# 과적합 현상을 방지하며 두 개의 RNN 셀을 이용합니다.\n",
    "cell1 = tf.nn.rnn_cell.BasicLSTMCell(256)\n",
    "cell1 = tf.nn.rnn_cell.DropoutWrapper(cell1, output_keep_prob=0.7)\n",
    "cell2 = tf.nn.rnn_cell.BasicLSTMCell(256)\n",
    "\n",
    "# 두 셀을 조합하여 심층 순환 신경망을 구축합니다.\n",
    "multi_cell = tf.nn.rnn_cell.MultiRNNCell([cell1, cell2])\n",
    "outputs, states = tf.nn.dynamic_rnn(multi_cell, X, dtype=tf.float32)\n",
    "\n",
    "# 최종 출력층을 구축합니다.\n",
    "outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "outputs = outputs[-1]\n",
    "model = tf.matmul(outputs, W) + b\n",
    "\n",
    "# 인덱스 값을 그대로 결과로 사용할 수 있도록 비용 함수를 적용해 최적화합니다.\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                      logits=model, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "반복: 0001 비용: 3.5806\n",
      "반복: 0002 비용: 1.9336\n",
      "반복: 0003 비용: 1.5777\n",
      "반복: 0004 비용: 1.7899\n",
      "반복: 0005 비용: 1.4384\n",
      "반복: 0006 비용: 0.9035\n",
      "반복: 0007 비용: 0.4830\n",
      "반복: 0008 비용: 0.4160\n",
      "반복: 0009 비용: 0.3685\n",
      "반복: 0010 비용: 0.3434\n",
      "반복: 0011 비용: 0.3259\n",
      "반복: 0012 비용: 0.1758\n",
      "반복: 0013 비용: 0.1939\n",
      "반복: 0014 비용: 0.1842\n",
      "반복: 0015 비용: 0.2459\n",
      "반복: 0016 비용: 0.1581\n",
      "반복: 0017 비용: 0.1205\n",
      "반복: 0018 비용: 0.1478\n",
      "반복: 0019 비용: 0.1340\n",
      "반복: 0020 비용: 0.1642\n",
      "반복: 0021 비용: 0.1244\n",
      "반복: 0022 비용: 0.1687\n",
      "반복: 0023 비용: 0.1275\n",
      "반복: 0024 비용: 0.0862\n",
      "반복: 0025 비용: 0.1553\n",
      "반복: 0026 비용: 0.1347\n",
      "반복: 0027 비용: 0.1085\n",
      "반복: 0028 비용: 0.1755\n",
      "반복: 0029 비용: 0.1471\n",
      "반복: 0030 비용: 0.1732\n",
      "반복: 0031 비용: 0.1069\n",
      "반복: 0032 비용: 0.1314\n",
      "반복: 0033 비용: 0.0865\n",
      "반복: 0034 비용: 0.1384\n",
      "반복: 0035 비용: 0.1887\n",
      "반복: 0036 비용: 0.1037\n",
      "반복: 0037 비용: 0.2041\n",
      "반복: 0038 비용: 0.1435\n",
      "반복: 0039 비용: 0.2380\n",
      "반복: 0040 비용: 0.2671\n",
      "반복: 0041 비용: 0.1563\n",
      "반복: 0042 비용: 0.1584\n",
      "반복: 0043 비용: 0.1103\n",
      "반복: 0044 비용: 0.1792\n",
      "반복: 0045 비용: 0.1173\n",
      "반복: 0046 비용: 0.1527\n",
      "반복: 0047 비용: 0.1207\n",
      "반복: 0048 비용: 0.2257\n",
      "반복: 0049 비용: 0.1033\n",
      "반복: 0050 비용: 0.1992\n",
      "반복: 0051 비용: 0.0928\n",
      "반복: 0052 비용: 0.1193\n",
      "반복: 0053 비용: 0.1759\n",
      "반복: 0054 비용: 0.1475\n",
      "반복: 0055 비용: 0.1242\n",
      "반복: 0056 비용: 0.1972\n",
      "반복: 0057 비용: 0.1603\n",
      "반복: 0058 비용: 0.0705\n",
      "반복: 0059 비용: 0.0780\n",
      "반복: 0060 비용: 0.2345\n",
      "반복: 0061 비용: 0.1998\n",
      "반복: 0062 비용: 0.1568\n",
      "반복: 0063 비용: 0.1733\n",
      "반복: 0064 비용: 0.1048\n",
      "반복: 0065 비용: 0.1785\n",
      "반복: 0066 비용: 0.0944\n",
      "반복: 0067 비용: 0.1821\n",
      "반복: 0068 비용: 0.1758\n",
      "반복: 0069 비용: 0.1532\n",
      "반복: 0070 비용: 0.1220\n",
      "반복: 0071 비용: 0.1281\n",
      "반복: 0072 비용: 0.1133\n",
      "반복: 0073 비용: 0.0929\n",
      "반복: 0074 비용: 0.0611\n",
      "반복: 0075 비용: 0.1536\n",
      "반복: 0076 비용: 0.0879\n",
      "반복: 0077 비용: 0.2192\n",
      "반복: 0078 비용: 0.1641\n",
      "반복: 0079 비용: 0.1597\n",
      "반복: 0080 비용: 0.2053\n",
      "반복: 0081 비용: 0.1631\n",
      "반복: 0082 비용: 0.1216\n",
      "반복: 0083 비용: 0.1876\n",
      "반복: 0084 비용: 0.1355\n",
      "반복: 0085 비용: 0.1283\n",
      "반복: 0086 비용: 0.1411\n",
      "반복: 0087 비용: 0.0940\n",
      "반복: 0088 비용: 0.1257\n",
      "반복: 0089 비용: 0.1226\n",
      "반복: 0090 비용: 0.2682\n",
      "반복: 0091 비용: 0.0946\n",
      "반복: 0092 비용: 0.2111\n",
      "반복: 0093 비용: 0.1337\n",
      "반복: 0094 비용: 0.1300\n",
      "반복: 0095 비용: 0.0973\n",
      "반복: 0096 비용: 0.1543\n",
      "반복: 0097 비용: 0.1219\n",
      "반복: 0098 비용: 0.1374\n",
      "반복: 0099 비용: 0.1566\n",
      "반복: 0100 비용: 0.1185\n",
      "학습 완료!\n"
     ]
    }
   ],
   "source": [
    "''' RNN 신경망 모델 학습하기 '''\n",
    "\n",
    "# 그래프의 동작을 위해 세션을 초기화합니다.\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 입력 값과 출력 값으로 단어를 분리합니다.\n",
    "input_batch, target_batch = make_batch(seq_data)\n",
    "\n",
    "# 10번 반복하여 학습합니다.\n",
    "for epoch in range(100):\n",
    "    _, loss = sess.run([optimizer, cost],\n",
    "                       feed_dict={X: input_batch, Y: target_batch})\n",
    "    \n",
    "    print('반복:', '%04d' % (epoch + 1), '비용:', '{:.4f}'.format(loss))\n",
    "\n",
    "print('학습 완료!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력값: ['suc ', 'cas ', 'dar ', 'due ', 'lat ', 'car ', 'hal ', 'jav ', 'fel ', 'non ', 'dar ', 'doo ', 'dar ', 'dar ', 'sqr ', 'the ', 'the ', 'her ']\n",
      "예측값: ['such', 'case', 'dark', 'duel', 'late', 'carp', 'hall', 'java', 'fell', 'none', 'dark', 'doom', 'dare', 'dark', 'sqrt', 'them', 'them', 'here']\n",
      "정확도: 0.8888889\n"
     ]
    }
   ],
   "source": [
    "''' RNN 모델 결과 확인하기 '''\n",
    "\n",
    "prediction = tf.cast(tf.argmax(model, 1), tf.int32)\n",
    "# 출력값 Y는 인덱스 그대로 사용합니다.\n",
    "prediction_check = tf.equal(prediction, Y)\n",
    "accuracy = tf.reduce_mean(tf.cast(prediction_check, tf.float32))\n",
    "\n",
    "# 학습에 사용한 단어를 넣고 예측 모델을 동작시킵니다.\n",
    "input_batch, target_batch = make_batch(seq_data)\n",
    "\n",
    "predict, accuracy_val = sess.run([prediction, accuracy],\n",
    "                                 feed_dict={X: input_batch, Y: target_batch})\n",
    "\n",
    "predict_words = []\n",
    "for index, value in enumerate(seq_data):\n",
    "    # 예측 모델이 예측한 마지막 단어를 저장합니다.\n",
    "    last_char = char_arr[predict[index]]\n",
    "    # 기존의 세 단어와 예측 단어를 합쳐 출력합니다.\n",
    "    predict_words.append(value[:3] + last_char)\n",
    "    \n",
    "print('입력값:', [w[:3] + ' ' for w in seq_data])\n",
    "print('예측값:', predict_words)\n",
    "print('정확도:', accuracy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
